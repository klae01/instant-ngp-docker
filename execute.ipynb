{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload your data\n",
    "---------------\n",
    "- upload single file to at path `/workspace`\n",
    "- If you try new example, delete old file\n",
    "\n",
    "### case 1, upload by mp4 (video)\n",
    "- upload `<mp4 file name>.mp4` at `/workspace`\n",
    "\n",
    "### case 2, upload by zip (images, compress multiple PNG or JPG)\n",
    "- upload `<zip file name>.zip` at `/workspace`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add instant-ngp to python path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content\n",
    "import sys\n",
    "sys.path.append(\"instant-ngp\")\n",
    "sys.path.append(\"instant-ngp/build\")\n",
    "sys.path.append(\"instant-ngp/scripts\")\n",
    "import os\n",
    "os.environ[\"DISPLAY\"] = \":1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# colmap to nerf (MUST CHANGE FULL CODE BEFORE RELEASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from pathlib import Path, PurePosixPath\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "import math\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "def variance_of_laplacian(image):\n",
    "\treturn cv2.Laplacian(image, cv2.CV_64F).var()\n",
    "\n",
    "def sharpness(imagePath):\n",
    "\timage = cv2.imread(imagePath)\n",
    "\tgray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\tfm = variance_of_laplacian(gray)\n",
    "\treturn fm\n",
    "\n",
    "def qvec2rotmat(qvec):\n",
    "\treturn np.array([\n",
    "\t\t[\n",
    "\t\t\t1 - 2 * qvec[2]**2 - 2 * qvec[3]**2,\n",
    "\t\t\t2 * qvec[1] * qvec[2] - 2 * qvec[0] * qvec[3],\n",
    "\t\t\t2 * qvec[3] * qvec[1] + 2 * qvec[0] * qvec[2]\n",
    "\t\t], [\n",
    "\t\t\t2 * qvec[1] * qvec[2] + 2 * qvec[0] * qvec[3],\n",
    "\t\t\t1 - 2 * qvec[1]**2 - 2 * qvec[3]**2,\n",
    "\t\t\t2 * qvec[2] * qvec[3] - 2 * qvec[0] * qvec[1]\n",
    "\t\t], [\n",
    "\t\t\t2 * qvec[3] * qvec[1] - 2 * qvec[0] * qvec[2],\n",
    "\t\t\t2 * qvec[2] * qvec[3] + 2 * qvec[0] * qvec[1],\n",
    "\t\t\t1 - 2 * qvec[1]**2 - 2 * qvec[2]**2\n",
    "\t\t]\n",
    "\t])\n",
    "\n",
    "def rotmat(a, b):\n",
    "\ta, b = a / np.linalg.norm(a), b / np.linalg.norm(b)\n",
    "\tv = np.cross(a, b)\n",
    "\tc = np.dot(a, b)\n",
    "\t# handle exception for the opposite direction input\n",
    "\tif c < -1 + 1e-10:\n",
    "\t\treturn rotmat(a + np.random.uniform(-1e-2, 1e-2, 3), b)\n",
    "\ts = np.linalg.norm(v)\n",
    "\tkmat = np.array([[0, -v[2], v[1]], [v[2], 0, -v[0]], [-v[1], v[0], 0]])\n",
    "\treturn np.eye(3) + kmat + kmat.dot(kmat) * ((1 - c) / (s ** 2 + 1e-10))\n",
    "\n",
    "def closest_point_2_lines(oa, da, ob, db): # returns point closest to both rays of form o+t*d, and a weight factor that goes to 0 if the lines are parallel\n",
    "\tda = da / np.linalg.norm(da)\n",
    "\tdb = db / np.linalg.norm(db)\n",
    "\tc = np.cross(da, db)\n",
    "\tdenom = np.linalg.norm(c)**2\n",
    "\tt = ob - oa\n",
    "\tta = np.linalg.det([t, db, c]) / (denom + 1e-10)\n",
    "\ttb = np.linalg.det([t, da, c]) / (denom + 1e-10)\n",
    "\tif ta > 0:\n",
    "\t\tta = 0\n",
    "\tif tb > 0:\n",
    "\t\ttb = 0\n",
    "\treturn (oa+ta*da+ob+tb*db) * 0.5, denom\n",
    "\n",
    "def colmap2nerf(args):\n",
    "\tparser = argparse.ArgumentParser(description=\"convert a text colmap export to nerf format transforms.json; optionally convert video to images, and optionally run colmap in the first place\")\n",
    "\n",
    "\tparser.add_argument(\"--video_in\", default=\"\", help=\"run ffmpeg first to convert a provided video file into a set of images. uses the video_fps parameter also\")\n",
    "\tparser.add_argument(\"--video_fps\", default=2)\n",
    "\tparser.add_argument(\"--time_slice\", default=\"\", help=\"time (in seconds) in the format t1,t2 within which the images should be generated from the video. eg: \\\"--time_slice '10,300'\\\" will generate images only from 10th second to 300th second of the video\")\n",
    "\tparser.add_argument(\"--run_colmap\", action=\"store_true\", help=\"run colmap first on the image folder\")\n",
    "\tparser.add_argument(\"--colmap_matcher\", default=\"sequential\", choices=[\"exhaustive\",\"sequential\",\"spatial\",\"transitive\",\"vocab_tree\"], help=\"select which matcher colmap should use. sequential for videos, exhaustive for adhoc images\")\n",
    "\tparser.add_argument(\"--colmap_db\", default=\"colmap.db\", help=\"colmap database filename\")\n",
    "\tparser.add_argument(\"--colmap_camera_model\", default=\"OPENCV\", choices=[\"SIMPLE_PINHOLE\", \"PINHOLE\", \"SIMPLE_RADIAL\", \"RADIAL\",\"OPENCV\"], help=\"camera model\")\n",
    "\tparser.add_argument(\"--colmap_camera_params\", default=\"\", help=\"intrinsic parameters, depending on the chosen model.  Format: fx,fy,cx,cy,dist\")\n",
    "\tparser.add_argument(\"--images\", default=\"images\", help=\"input path to the images\")\n",
    "\tparser.add_argument(\"--text\", default=\"colmap_text\", help=\"input path to the colmap text files (set automatically if run_colmap is used)\")\n",
    "\tparser.add_argument(\"--aabb_scale\", default=16, choices=[\"1\",\"2\",\"4\",\"8\",\"16\"], help=\"large scene scale factor. 1=scene fits in unit cube; power of 2 up to 16\")\n",
    "\tparser.add_argument(\"--skip_early\", default=0, help=\"skip this many images from the start\")\n",
    "\tparser.add_argument(\"--keep_colmap_coords\", action=\"store_true\", help=\"keep transforms.json in COLMAP's original frame of reference (this will avoid reorienting and repositioning the scene for preview and rendering)\")\n",
    "\tparser.add_argument(\"--out\", default=\"transforms.json\", help=\"output path\")\n",
    "\targs = parser.parse_args(args.split())\n",
    "\tAABB_SCALE = int(args.aabb_scale)\n",
    "\tSKIP_EARLY = int(args.skip_early)\n",
    "\tIMAGE_FOLDER = args.images\n",
    "\tTEXT_FOLDER = args.text\n",
    "\tOUT_PATH = args.out\n",
    "\tprint(f\"outputting to {OUT_PATH}...\")\n",
    "\twith open(os.path.join(TEXT_FOLDER,\"cameras.txt\"), \"r\") as f:\n",
    "\t\tangle_x = math.pi / 2\n",
    "\t\tfor line in f:\n",
    "\t\t\t# 1 SIMPLE_RADIAL 2048 1536 1580.46 1024 768 0.0045691\n",
    "\t\t\t# 1 OPENCV 3840 2160 3178.27 3182.09 1920 1080 0.159668 -0.231286 -0.00123982 0.00272224\n",
    "\t\t\t# 1 RADIAL 1920 1080 1665.1 960 540 0.0672856 -0.0761443\n",
    "\t\t\tif line[0] == \"#\":\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tels = line.split(\" \")\n",
    "\t\t\tw = float(els[2])\n",
    "\t\t\th = float(els[3])\n",
    "\t\t\tfl_x = float(els[4])\n",
    "\t\t\tfl_y = float(els[4])\n",
    "\t\t\tk1 = 0\n",
    "\t\t\tk2 = 0\n",
    "\t\t\tp1 = 0\n",
    "\t\t\tp2 = 0\n",
    "\t\t\tcx = w / 2\n",
    "\t\t\tcy = h / 2\n",
    "\t\t\tif els[1] == \"SIMPLE_PINHOLE\":\n",
    "\t\t\t\tcx = float(els[5])\n",
    "\t\t\t\tcy = float(els[6])\n",
    "\t\t\telif els[1] == \"PINHOLE\":\n",
    "\t\t\t\tfl_y = float(els[5])\n",
    "\t\t\t\tcx = float(els[6])\n",
    "\t\t\t\tcy = float(els[7])\n",
    "\t\t\telif els[1] == \"SIMPLE_RADIAL\":\n",
    "\t\t\t\tcx = float(els[5])\n",
    "\t\t\t\tcy = float(els[6])\n",
    "\t\t\t\tk1 = float(els[7])\n",
    "\t\t\telif els[1] == \"RADIAL\":\n",
    "\t\t\t\tcx = float(els[5])\n",
    "\t\t\t\tcy = float(els[6])\n",
    "\t\t\t\tk1 = float(els[7])\n",
    "\t\t\t\tk2 = float(els[8])\n",
    "\t\t\telif els[1] == \"OPENCV\":\n",
    "\t\t\t\tfl_y = float(els[5])\n",
    "\t\t\t\tcx = float(els[6])\n",
    "\t\t\t\tcy = float(els[7])\n",
    "\t\t\t\tk1 = float(els[8])\n",
    "\t\t\t\tk2 = float(els[9])\n",
    "\t\t\t\tp1 = float(els[10])\n",
    "\t\t\t\tp2 = float(els[11])\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(\"unknown camera model \", els[1])\n",
    "\t\t\t# fl = 0.5 * w / tan(0.5 * angle_x);\n",
    "\t\t\tangle_x = math.atan(w / (fl_x * 2)) * 2\n",
    "\t\t\tangle_y = math.atan(h / (fl_y * 2)) * 2\n",
    "\t\t\tfovx = angle_x * 180 / math.pi\n",
    "\t\t\tfovy = angle_y * 180 / math.pi\n",
    "\n",
    "\tprint(f\"camera:\\n\\tres={w,h}\\n\\tcenter={cx,cy}\\n\\tfocal={fl_x,fl_y}\\n\\tfov={fovx,fovy}\\n\\tk={k1,k2} p={p1,p2} \")\n",
    "\n",
    "\twith open(os.path.join(TEXT_FOLDER,\"images.txt\"), \"r\") as f:\n",
    "\t\ti = 0\n",
    "\t\tbottom = np.array([0.0, 0.0, 0.0, 1.0]).reshape([1, 4])\n",
    "\t\tout = {\n",
    "\t\t\t\"camera_angle_x\": angle_x,\n",
    "\t\t\t\"camera_angle_y\": angle_y,\n",
    "\t\t\t\"fl_x\": fl_x,\n",
    "\t\t\t\"fl_y\": fl_y,\n",
    "\t\t\t\"k1\": k1,\n",
    "\t\t\t\"k2\": k2,\n",
    "\t\t\t\"p1\": p1,\n",
    "\t\t\t\"p2\": p2,\n",
    "\t\t\t\"cx\": cx,\n",
    "\t\t\t\"cy\": cy,\n",
    "\t\t\t\"w\": w,\n",
    "\t\t\t\"h\": h,\n",
    "\t\t\t\"aabb_scale\": AABB_SCALE,\n",
    "\t\t\t\"frames\": [],\n",
    "\t\t}\n",
    "\n",
    "\t\tup = np.zeros(3)\n",
    "\t\tfor line in f:\n",
    "\t\t\tline = line.strip()\n",
    "\t\t\tif line[0] == \"#\":\n",
    "\t\t\t\tcontinue\n",
    "\t\t\ti = i + 1\n",
    "\t\t\tif i < SKIP_EARLY*2:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif  i % 2 == 1:\n",
    "\t\t\t\telems=line.split(\" \") # 1-4 is quat, 5-7 is trans, 9ff is filename (9, if filename contains no spaces)\n",
    "\t\t\t\t#name = str(PurePosixPath(Path(IMAGE_FOLDER, elems[9])))\n",
    "\t\t\t\t# why is this requireing a relitive path while using ^\n",
    "\t\t\t\timage_rel = os.path.relpath(IMAGE_FOLDER)\n",
    "\t\t\t\tname = str(f\"./{image_rel}/{'_'.join(elems[9:])}\")\n",
    "\t\t\t\tb=sharpness(name)\n",
    "\t\t\t\tprint(name, \"sharpness=\",b)\n",
    "\t\t\t\timage_id = int(elems[0])\n",
    "\t\t\t\tqvec = np.array(tuple(map(float, elems[1:5])))\n",
    "\t\t\t\ttvec = np.array(tuple(map(float, elems[5:8])))\n",
    "\t\t\t\tR = qvec2rotmat(-qvec)\n",
    "\t\t\t\tt = tvec.reshape([3,1])\n",
    "\t\t\t\tm = np.concatenate([np.concatenate([R, t], 1), bottom], 0)\n",
    "\t\t\t\tc2w = np.linalg.inv(m)\n",
    "\t\t\t\tif not args.keep_colmap_coords:\n",
    "\t\t\t\t\tc2w[0:3,2] *= -1 # flip the y and z axis\n",
    "\t\t\t\t\tc2w[0:3,1] *= -1\n",
    "\t\t\t\t\tc2w = c2w[[1,0,2,3],:] # swap y and z\n",
    "\t\t\t\t\tc2w[2,:] *= -1 # flip whole world upside down\n",
    "\n",
    "\t\t\t\t\tup += c2w[0:3,1]\n",
    "\n",
    "\t\t\t\tframe={\"file_path\":name,\"sharpness\":b,\"transform_matrix\": c2w}\n",
    "\t\t\t\tout[\"frames\"].append(frame)\n",
    "\tnframes = len(out[\"frames\"])\n",
    "\n",
    "\tif args.keep_colmap_coords:\n",
    "\t\tflip_mat = np.array([\n",
    "\t\t\t[1, 0, 0, 0],\n",
    "\t\t\t[0, -1, 0, 0],\n",
    "\t\t\t[0, 0, -1, 0],\n",
    "\t\t\t[0, 0, 0, 1]\n",
    "\t\t])\n",
    "\n",
    "\t\tfor f in out[\"frames\"]:\n",
    "\t\t\tf[\"transform_matrix\"] = np.matmul(f[\"transform_matrix\"], flip_mat) # flip cameras (it just works)\n",
    "\telse:\n",
    "\t\t# don't keep colmap coords - reorient the scene to be easier to work with\n",
    "\n",
    "\t\tup = up / np.linalg.norm(up)\n",
    "\t\tprint(\"up vector was\", up)\n",
    "\t\tR = rotmat(up,[0,0,1]) # rotate up vector to [0,0,1]\n",
    "\t\tR = np.pad(R,[0,1])\n",
    "\t\tR[-1, -1] = 1\n",
    "\n",
    "\t\tfor f in out[\"frames\"]:\n",
    "\t\t\tf[\"transform_matrix\"] = np.matmul(R, f[\"transform_matrix\"]) # rotate up to be the z axis\n",
    "\n",
    "\t\t# find a central point they are all looking at\n",
    "\t\tprint(\"computing center of attention...\")\n",
    "\t\ttotw = 0.0\n",
    "\t\ttotp = np.array([0.0, 0.0, 0.0])\n",
    "\t\tfor f in out[\"frames\"]:\n",
    "\t\t\tmf = f[\"transform_matrix\"][0:3,:]\n",
    "\t\t\tfor g in out[\"frames\"]:\n",
    "\t\t\t\tmg = g[\"transform_matrix\"][0:3,:]\n",
    "\t\t\t\tp, w = closest_point_2_lines(mf[:,3], mf[:,2], mg[:,3], mg[:,2])\n",
    "\t\t\t\tif w > 0.01:\n",
    "\t\t\t\t\ttotp += p*w\n",
    "\t\t\t\t\ttotw += w\n",
    "\t\ttotp /= totw\n",
    "\t\tprint(totp) # the cameras are looking at totp\n",
    "\t\tfor f in out[\"frames\"]:\n",
    "\t\t\tf[\"transform_matrix\"][0:3,3] -= totp\n",
    "\n",
    "\t\tavglen = 0.\n",
    "\t\tfor f in out[\"frames\"]:\n",
    "\t\t\tavglen += np.linalg.norm(f[\"transform_matrix\"][0:3,3])\n",
    "\t\tavglen /= nframes\n",
    "\t\tprint(\"avg camera distance from origin\", avglen)\n",
    "\t\tfor f in out[\"frames\"]:\n",
    "\t\t\tf[\"transform_matrix\"][0:3,3] *= 4.0 / avglen # scale to \"nerf sized\"\n",
    "\n",
    "\tfor f in out[\"frames\"]:\n",
    "\t\tf[\"transform_matrix\"] = f[\"transform_matrix\"].tolist()\n",
    "\tprint(nframes,\"frames\")\n",
    "\tprint(f\"writing {OUT_PATH}\")\n",
    "\twith open(OUT_PATH, \"w\") as outfile:\n",
    "\t\tjson.dump(out, outfile, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ngp training iteration and control API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import os\n",
    "import commentjson as cjson\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "from common import *\n",
    "from scenes import scenes_nerf\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pyngp as ngp # noqa\n",
    "\n",
    "\n",
    "def training_step(args):\n",
    "\tdef control_API(key, value):\n",
    "\t\tnonlocal testbed\n",
    "\t\tif key  == \"position\":\n",
    "\t\t\ttestbed.position = value\n",
    "\t\telif key == \"zoom\":\n",
    "\t\t\ttestbed.zoom = value\n",
    "\t\telif key == \"render_aabb\":\n",
    "\t\t\ttestbed.render_aabb=ngp.BoundingBox(value[0], value[1])\n",
    "\t\telif key == \"visualize_cameras\":\n",
    "\t\t\ttestbed.nerf.visualize_cameras = value\n",
    "\t\telif key == \"background_color\":\n",
    "\t\t\ttestbed.background_color = value\n",
    "\t\telif key == \"random_bg_color\":\n",
    "\t\t\ttestbed.nerf.training.random_bg_color = value\n",
    "\t\telse:\n",
    "\t\t\traise NotImplementedError()\n",
    "\n",
    "\tdef get_render():\n",
    "\t\tnonlocal testbed\n",
    "\t\treturn testbed.render(args.width, args.height, args.screenshot_spp, True), testbed.training_step\n",
    "\n",
    "\tdef get_testbed():\n",
    "\t\tnonlocal testbed\n",
    "\t\treturn testbed\n",
    "\t\n",
    "\tparser = argparse.ArgumentParser(description=\"Run neural graphics primitives testbed with additional configuration & output options\")\n",
    "\n",
    "\tparser.add_argument(\"--scene\", \"--training_data\", default=\"\", help=\"The scene to load. Can be the scene's name or a full path to the training data.\")\n",
    "\tparser.add_argument(\"--mode\", default=\"\", const=\"nerf\", nargs=\"?\", choices=[\"nerf\", \"sdf\", \"image\", \"volume\"], help=\"Mode can be 'nerf', 'sdf', 'image' or 'volume'. Inferred from the scene if unspecified.\")\n",
    "\tparser.add_argument(\"--network\", default=\"\", help=\"Path to the network config. Uses the scene's default if unspecified.\")\n",
    "\n",
    "\tparser.add_argument(\"--load_snapshot\", default=\"\", help=\"Load this snapshot before training. recommended extension: .msgpack\")\n",
    "\tparser.add_argument(\"--save_snapshot\", default=\"\", help=\"Save this snapshot after training. recommended extension: .msgpack\")\n",
    "\n",
    "\tparser.add_argument(\"--nerf_compatibility\", action=\"store_true\", help=\"Matches parameters with original NeRF. Can cause slowness and worse results on some scenes.\")\n",
    "\tparser.add_argument(\"--test_transforms\", default=\"\", help=\"Path to a nerf style transforms json from which we will compute PSNR.\")\n",
    "\tparser.add_argument(\"--near_distance\", default=-1, type=float, help=\"Set the distance from the camera at which training rays start for nerf. <0 means use ngp default\")\n",
    "\tparser.add_argument(\"--exposure\", default=0.0, type=float, help=\"Controls the brightness of the image. Positive numbers increase brightness, negative numbers decrease it.\")\n",
    "\n",
    "\tparser.add_argument(\"--screenshot_transforms\", default=\"\", help=\"Path to a nerf style transforms.json from which to save screenshots.\")\n",
    "\tparser.add_argument(\"--screenshot_frames\", nargs=\"*\", help=\"Which frame(s) to take screenshots of.\")\n",
    "\tparser.add_argument(\"--screenshot_dir\", default=\"\", help=\"Which directory to output screenshots to.\")\n",
    "\tparser.add_argument(\"--screenshot_spp\", type=int, default=16, help=\"Number of samples per pixel in screenshots.\")\n",
    "\n",
    "\tparser.add_argument(\"--video_camera_path\", default=\"\", help=\"The camera path to render.\")\n",
    "\tparser.add_argument(\"--video_camera_smoothing\", action=\"store_true\", help=\"Applies additional smoothing to the camera trajectory with the caveat that the endpoint of the camera path may not be reached.\")\n",
    "\tparser.add_argument(\"--video_fps\", type=int, default=60, help=\"Number of frames per second.\")\n",
    "\tparser.add_argument(\"--video_n_seconds\", type=int, default=1, help=\"Number of seconds the rendered video should be long.\")\n",
    "\tparser.add_argument(\"--video_spp\", type=int, default=8, help=\"Number of samples per pixel. A larger number means less noise, but slower rendering.\")\n",
    "\tparser.add_argument(\"--video_output\", type=str, default=\"video.mp4\", help=\"Filename of the output video.\")\n",
    "\n",
    "\tparser.add_argument(\"--save_mesh\", default=\"\", help=\"Output a marching-cubes based mesh from the NeRF or SDF model. Supports OBJ and PLY format.\")\n",
    "\tparser.add_argument(\"--marching_cubes_res\", default=256, type=int, help=\"Sets the resolution for the marching cubes grid.\")\n",
    "\n",
    "\tparser.add_argument(\"--width\", \"--screenshot_w\", type=int, default=1920, help=\"Resolution width of GUI and screenshots.\")\n",
    "\tparser.add_argument(\"--height\", \"--screenshot_h\", type=int, default=1080, help=\"Resolution height of GUI and screenshots.\")\n",
    "\n",
    "\tparser.add_argument(\"--gui\", action=\"store_true\", help=\"Run the testbed GUI interactively.\")\n",
    "\tparser.add_argument(\"--train\", action=\"store_true\", help=\"If the GUI is enabled, controls whether training starts immediately.\")\n",
    "\tparser.add_argument(\"--n_steps\", type=int, default=-1, help=\"Number of steps to train for before quitting.\")\n",
    "\n",
    "\tparser.add_argument(\"--sharpen\", default=0, help=\"Set amount of sharpening applied to NeRF training images.\")\n",
    "\n",
    "\n",
    "\targs = parser.parse_args(args.split())\n",
    "\n",
    "\tmode = ngp.TestbedMode.Nerf\n",
    "\tconfigs_dir = os.path.join(ROOT_DIR, \"configs\", \"nerf\")\n",
    "\tscenes = scenes_nerf\n",
    "\n",
    "\tbase_network = os.path.join(configs_dir, \"base.json\")\n",
    "\tif args.scene in scenes:\n",
    "\t\tnetwork = scenes[args.scene][\"network\"] if \"network\" in scenes[args.scene] else \"base\"\n",
    "\t\tbase_network = os.path.join(configs_dir, network+\".json\")\n",
    "\tnetwork = args.network if args.network else base_network\n",
    "\tif not os.path.isabs(network):\n",
    "\t\tnetwork = os.path.join(configs_dir, network)\n",
    "\n",
    "\ttestbed = ngp.Testbed(mode)\n",
    "\ttestbed.nerf.sharpen = float(args.sharpen)\n",
    "\ttestbed.exposure = args.exposure\n",
    "\n",
    "\tif args.scene:\n",
    "\t\tscene = args.scene\n",
    "\t\tif not os.path.exists(args.scene) and args.scene in scenes:\n",
    "\t\t\tscene = os.path.join(scenes[args.scene][\"data_dir\"], scenes[args.scene][\"dataset\"])\n",
    "\t\ttestbed.load_training_data(scene)\n",
    "\n",
    "\tif args.gui:\n",
    "\t\t# Pick a sensible GUI resolution depending on arguments.\n",
    "\t\tsw = args.width\n",
    "\t\tsh = args.height\n",
    "\t\twhile sw*sh > 1920*1080*4:\n",
    "\t\t\tsw = int(sw / 2)\n",
    "\t\t\tsh = int(sh / 2)\n",
    "\t\ttestbed.init_window(sw, sh)\n",
    "\n",
    "\n",
    "\tif args.load_snapshot:\n",
    "\t\tprint(\"Loading snapshot \", args.load_snapshot)\n",
    "\t\ttestbed.load_snapshot(args.load_snapshot)\n",
    "\telse:\n",
    "\t\ttestbed.reload_network_from_file(network)\n",
    "\n",
    "\ttestbed.shall_train = args.train if args.gui else True\n",
    "\n",
    "\n",
    "\ttestbed.nerf.render_with_camera_distortion = True\n",
    "\n",
    "\tnetwork_stem = os.path.splitext(os.path.basename(network))[0]\n",
    "\n",
    "\tif args.near_distance >= 0.0:\n",
    "\t\tprint(\"NeRF training ray near_distance \", args.near_distance)\n",
    "\t\ttestbed.nerf.training.near_distance = args.near_distance\n",
    "\n",
    "\told_training_step = 0\n",
    "\tn_steps = args.n_steps\n",
    "\n",
    "\t# If we loaded a snapshot, didn't specify a number of steps, _and_ didn't open a GUI,\n",
    "\t# don't train by default and instead assume that the goal is to render screenshots,\n",
    "\t# compute PSNR, or render a video.\n",
    "\tif n_steps < 0 and (not args.load_snapshot or args.gui):\n",
    "\t\tn_steps = 35000\n",
    "\t\n",
    "\tyield control_API\n",
    "\tyield get_render\n",
    "\tyield get_testbed\n",
    "\n",
    "\ttqdm_last_update = 0\n",
    "\tif n_steps > 0:\n",
    "\t\twith tqdm(desc=\"Training\", total=n_steps, unit=\"step\") as t:\n",
    "\t\t\twhile testbed.frame():\n",
    "\t\t\t\tif testbed.want_repl():\n",
    "\t\t\t\t\trepl(testbed)\n",
    "\t\t\t\t# What will happen when training is done?\n",
    "\t\t\t\tif testbed.training_step >= n_steps:\n",
    "\t\t\t\t\tif args.gui:\n",
    "\t\t\t\t\t\ttestbed.shall_train = False\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\t\t# Update progress bar\n",
    "\t\t\t\tif testbed.training_step < old_training_step or old_training_step == 0:\n",
    "\t\t\t\t\told_training_step = 0\n",
    "\t\t\t\t\tt.reset()\n",
    "\n",
    "\t\t\t\tnow = time.monotonic()\n",
    "\t\t\t\tif now - tqdm_last_update > 0.1:\n",
    "\t\t\t\t\tt.update(testbed.training_step - old_training_step)\n",
    "\t\t\t\t\tt.set_postfix(loss=testbed.loss)\n",
    "\t\t\t\t\told_training_step = testbed.training_step\n",
    "\t\t\t\t\ttqdm_last_update = now\n",
    "\t\t\t\t\tyield t.n\n",
    "\n",
    "\tif args.save_snapshot:\n",
    "\t\tprint(\"Saving snapshot \", args.save_snapshot)\n",
    "\t\ttestbed.save_snapshot(args.save_snapshot, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing (colmap convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content\n",
    "import glob\n",
    "import os\n",
    "\n",
    "!rm -r DATA\n",
    "!rm -r result\n",
    "!mkdir DATA\n",
    "!mkdir DATA/images\n",
    "!mkdir DATA/colmap_text\n",
    "!mkdir result\n",
    "\n",
    "if glob.glob(f\"*.mp4\"):\n",
    "    get_frames = 150\n",
    "    FRAMES=os.popen('ffprobe -v error -select_streams v:0 -count_packets -show_entries stream=nb_read_packets -of csv=p=0 *.mp4').read()\n",
    "    FRAMES=int(FRAMES)\n",
    "    FRAMES=set(round((i + 0.499)*FRAMES/get_frames) for i in range(get_frames))\n",
    "    vf_select = \"+\".join(map(lambda X: f\"eq(n\\\\,{X})\", FRAMES))\n",
    "    !ffmpeg -i *.mp4 -vf 'select={vf_select}' -vsync 0 DATA/images/%04d.png\n",
    "\n",
    "elif glob.glob(f\"*.zip\"):\n",
    "    !mkdir original\n",
    "    !mkdir images\n",
    "    !unzip *.zip -d original/\n",
    "    !mv original/**.jpg images\n",
    "    !mv original/**.png images\n",
    "    !mv original/**.JPG images\n",
    "    !mv original/**.PNG images\n",
    "    !mv original/**/*.jpg images\n",
    "    !mv original/**/*.png images\n",
    "    !mv original/**/*.JPG images\n",
    "    !mv original/**/*.PNG images\n",
    "    !for f in images/*\\ *; do mv \"$f\" DATA/\"${f// /_}\"; done\n",
    "    !rm -r original\n",
    "    !rm -r images\n",
    "\n",
    "!cd DATA \\\n",
    " && colmap automatic_reconstructor --random_seed 1010 \\\n",
    "    --workspace_path . \\\n",
    "    --image_path images --mask_path mask_images \\\n",
    "    --data_type video --quality high --camera_model SIMPLE_RADIAL \\\n",
    "    --sparse 1 --dense 0  > /dev/null \\\n",
    " && colmap model_converter --input_path sparse/0 --output_path colmap_text --output_type TXT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert to nerf format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/DATA\n",
    "colmap2nerf(\"--aabb_scale 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/instant-ngp\n",
    "step_iter = iter(training_step(\n",
    "    \"--train --n_steps 10000 --network base.json --mode nerf --scene /content/DATA \"\n",
    "    + \"--width 480 --height 360 --screenshot_spp 1\"))\n",
    "update_API = next(step_iter)\n",
    "get_render = next(step_iter)\n",
    "get_testbed = next(step_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple controler + renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import queue\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.animation\n",
    "\n",
    "f = plt.figure()\n",
    "ax = f.gca()\n",
    "\n",
    "state_collect = queue.Queue()\n",
    "def function_for_animation(frame_index):\n",
    "    img, steps = state_collect.get()\n",
    "    image.set_data(img)\n",
    "    ax.set_title(f\"{steps} steps\")\n",
    "    return image,\n",
    "\n",
    "\n",
    "import asyncio\n",
    "from ipywidgets import FloatSlider, Output\n",
    "slider = [\n",
    "    FloatSlider(min=-10,max=10,step=0.01,value=0, description=\"x\"), \n",
    "    FloatSlider(min=-10,max=10,step=0.01,value=0, description=\"y\"), \n",
    "    FloatSlider(min=-10,max=10,step=0.01,value=1, description=\"z\"), \n",
    "    FloatSlider(min=0,max=10,step=0.1,value=1, description=\"zoom\"), \n",
    "]\n",
    "for s in slider:\n",
    "    s._ipython_display_()\n",
    "\n",
    "def wait_for_change(widget, value):\n",
    "    future = asyncio.Future()\n",
    "    def getvalue(change):\n",
    "        # make the new value available\n",
    "        future.set_result(change.new)\n",
    "        widget.unobserve(getvalue, value)\n",
    "    widget.observe(getvalue, value)\n",
    "    return future\n",
    "async def g(widget, DB):\n",
    "    DB.update({widget.description: widget.value})\n",
    "    while 1:\n",
    "        x = await wait_for_change(widget, \"value\")\n",
    "        DB.update({widget.description: x})\n",
    "async def f(widgets):\n",
    "    global state_DB\n",
    "    for w in widgets:\n",
    "        asyncio.ensure_future(g(w, state_DB))\n",
    "state_DB = {}\n",
    "\n",
    "async def trainer(step_iter):\n",
    "    for _ in step_iter:\n",
    "        state_collect.put(get_render())\n",
    "        await asyncio.sleep(0.001)\n",
    "        update_API(\"position\", [state_DB[k] for k in \"xyz\"])\n",
    "        update_API(\"zoom\", state_DB[\"zoom\"])\n",
    "\n",
    "asyncio.ensure_future(trainer (step_iter))\n",
    "asyncio.ensure_future(f(slider))\n",
    "\n",
    "image = plt.imshow(state_collect.get()[0], interpolation='None', animated=True)\n",
    "ani = matplotlib.animation.FuncAnimation(f, function_for_animation, interval=100, frames=None, blit=True, cache_frame_data=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
